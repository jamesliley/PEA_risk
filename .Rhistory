})
library(ggplot2)
library(patchwork)
library(scales)
# Calibration curves ####
obj_name=paste0("cal_v",version,"_",names(groupings)[g])
xname=paste0(out_dir,obj_name,".pdf")
obj=get(obj_name)
with(obj,{
pname="cal"
labs=c("Overall",names(groupings[[g]]))
xcol=phs_colours(c("phs-blue","phs-magenta","phs-purple"))
cal_2panel(list(cal_overall,calA,calB),
labels = labs,
col=xcol,
highlight=highlight_value,
#yrange=plot_details[[input$pname]]$yrange,
#lpos=plot_details[[input$pname]]$lpos,
yrange_lower=plot_details[[pname]]$yrange_lower,
legend_title="Group")
})
source("~/Research/Fairness/Git/SPARRAfairness/SPARRAfairness/R/SPARRAfairness_functions.R")
cutoff=10/100 # 10% risk score threshold
version=3; g=1
obj_name=paste0("forp_decomposition_v",version,"_",names(groupings)[[g]])
xname=paste0(out_dir,obj_name,".pdf")
names_group1=paste0("v",version,"_",names(groupings[[g]])[1],"_q",1:20)
names_group2=paste0("v",version,"_",names(groupings[[g]])[2],"_q",1:20)
decomp1=decomposition_matrix[names_group1,]
decomp2=decomposition_matrix[names_group2,]
plot_decomp(decomp1,
decomp2,
threshold=cutoff,
labels=names(groupings[[g]]))
subgroup = names(groupings[[g]])[1]
subgroup
obj_name=paste0("for_breakdown_v",version,"_",subgroup)
xname=paste0(out_dir,obj_name,".pdf")
names_group=paste0("v",version,"_",subgroup,"_q",1:20)
decomp=decomposition_matrix[names_group,]
for_breakdown(decomp,group = subgroup,threshold = cutoff,ylimit=c(-0.065,0.065),ldiff=0.003)
library(ggrepel)
obj_name=paste0("for_breakdown_v",version,"_",subgroup)
xname=paste0(out_dir,obj_name,".pdf")
names_group=paste0("v",version,"_",subgroup,"_q",1:20)
decomp=decomposition_matrix[names_group,]
for_breakdown(decomp,group = subgroup,threshold = cutoff,ylimit=c(-0.065,0.065),ldiff=0.003)
obj_name=paste0("for_breakdown_v",version,"_",subgroup)
xname=paste0(out_dir,obj_name,".pdf")
names_group=paste0("v",version,"_",subgroup,"_q",1:20)
decomp=decomposition_matrix[names_group,]
for_breakdown(decomp,group = subgroup,threshold = cutoff,ylimit=c(-0.065,0.065),ldiff=0.003,colour_points = TRUE)
library(devtools)
library(devtools)
setwd("../SPARRAfairness/")
document()
setwd("SPARRAfairness/")
document()
setwd("..")
install("SPARRAfairness/")
1+1
getwd()
library(devtools)
install("SPARRAfairness/")
setwd("..")
setwd ("../Git/sparra-performance-analysis/")
source("~/Research/Fairness/Git/sparra-performance-analysis/Code/pipeline.R")
library(devtools)
setwd("../SPARRAfairness/")
install("SPARRAfairness/")
setwd("../../Git/sparra-performance-analysis/")
source("~/Research/Fairness/Git/sparra-performance-analysis/Code/pipeline.R")
2^40
setwd("~/Research/CTEPH/PEA_risk/Git/PEA_risk/")
## Implement necessary functions                                    ####
source("Code/auxiliary.R")
## Process raw data                                                 ####
source("Code/process_raw.R")
## Discovery analysis                                               ####
source("Code/discovery_analysis.R")
## Discovery analysis (low missingness)                             ####
low_missingness <<- TRUE
source("Code/discovery_analysis.R")
low_missingness <<- FALSE
# Output to Shiny
source("Code/outputs_for_shiny.R")
## Process raw data                                                 ####
source("Code/process_raw_prospective.R")
## Data location
datadir="../../Data/"
# Output location
outdir="Outputs/"
##**********************************************************************
## Packages and scripts                                             ####
##**********************************************************************
library(glmnet)
library(randomForest)
library(pROC)
source("Code/auxiliary.R") # Auxiliary functions
# Prospective data
load(paste0(datadir,"Temp_data/Design/prospective_table.RData"))
vtab0=vtab
dim(vtab)
colnames(vtab)[1:10]
nr=apply(vtab,2,function(x) length(which(is.na(x))))
length(nr)
plot(nr/502)
set.seed(327462)
Yc=vtab$surgeon_predicted_mortality_mean
# Remove exclusions
w=which((vtab$excluded==1|is.na(Yc)))
vtab=vtab[-w,]
Yc=Yc[-w]
# Establish time since PEA, if died.
ds=(as.Date(vtab$Death_date,format="%d/%m/%Y") - as.Date(vtab$pea_date,format="%d/%m/%Y"))
ds[which(!is.finite(ds))]=500
Yv=(ds<35) # Died <35 days after PEA
# Find number of missing values per column, and remove variables with  missingness > 0.5
nm=apply(vtab,2,function(x) length(which(is.na(x))))/dim(vtab)[1]
subm=colnames(vtab)[which(nm < 0.5)]
# Training data
Y=YDM
length(Y)
dim(Xall)
train_dat=cbind(Xall,Y=as.factor(Y))
nr=50 # Size to test
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,xcol]
##**********************************************************************
## Sample size calculation                                          ####
##**********************************************************************
# Training data
Y=YDM
train_dat=cbind(Xall,Y=as.factor(Y))
nr=50 # Size to test
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,xcol]
sim_disc=sim_disc[,xcol]
help(wilcox.test)
##**********************************************************************
## Set data and output directories                                  ####
##**********************************************************************
## Data location
datadir="../../Data/"
# Output location
outdir="Outputs/"
##**********************************************************************
## Packages and scripts                                             ####
##**********************************************************************
library(glmnet)
library(randomForest)
library(pROC)
source("Code/auxiliary.R") # Auxiliary functions
##**********************************************************************
## 3. PROSPECTIVE VALIDATION                                        ####
## Read datasets                                                    ####
##**********************************************************************
# Prospective data
load(paste0(datadir,"Temp_data/Design/prospective_table.RData"))
vtab0=vtab
# Discovery
load(paste0(datadir,"Temp_data/Workspaces/discovery.RData"))
##**********************************************************************
## Sample size calculation setup                                    ####
##**********************************************************************
# Training data
Y=YDM
train_dat=cbind(Xall,Y=as.factor(Y))
nrs=c(10,200) # Maximum and minimum sizes to test
nsim=100 # Simulate this many times
# Store simulation data in a list
sim_data=list()
##**********************************************************************
## Sample size calculation                                          ####
##**********************************************************************
# Function to run test for one potential sample size
test_n=function(nr) {
for (ss in 1:nsim) {
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,xcol]
sim_disc=sim_disc[,xcol]
# Fit RF
nt=round(dim(sim_disc)[1]/2)
m_sim=randomForest(Y~.,data=sim_disc)
# Predictions on simulated validation sample
Ypred_sim=predict(m1,sim_val,type="prob")[,1]
# Wilcoxon test - does the model perform better than randomly?
wt_sim=wilcox.test(Ypred_sim[which(sim_val$Y==1)],Ypred_sim[which(sim_val$Y==0)])
npoll[ss]=wt_sim$p.value
}
return(npoll)
}
# Run bisection method to find sample size for 90% power
power_threshold=0.9
lower=nrs[1]
upper=nrs[2]
p_lower=test_n(lower)
p_upper=test_n(upper)
nr=50
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,xcol]
sim_disc=sim_disc[,xcol]
# Fit RF
nt=round(dim(sim_disc)[1]/2)
m_sim=randomForest(Y~.,data=sim_disc)
# Predictions on simulated validation sample
Ypred_sim=predict(m1,sim_val,type="prob")[,1]
class(sim_disc)
m_sim=randomForest(Y~.,data=sim_disc)
colnames(sim_disc)
length(cboot)
dim(train_dat)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
dim(sim_disc)
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm < 0.5)]
lenght(subm_val)
length(subm_val)
dim(sim_val)
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm_val < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,xcol]
sim_disc=sim_disc[,xcol]
dim(sim_val)
dim(sim_disc)
# Fit RF
nt=round(dim(sim_disc)[1]/2)
m_sim=randomForest(Y~.,data=sim_disc)
# Predictions on simulated validation sample
Ypred_sim=predict(m1,sim_val,type="prob")[,1]
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm_val < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
sim_disc[which(is.na(sim_disc[,i])),i]=mtab[colnames(sim_disc)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,xcol]
sim_disc=sim_disc[,xcol]
# Fit RF
nt=round(dim(sim_disc)[1]/2)
m_sim=randomForest(Y~.,data=sim_disc)
# Predictions on simulated validation sample
Ypred_sim=predict(m1,sim_val,type="prob")[,1]
length(which(is.na(sim_disc)))
dim(sim_disc)
length(sim_disc$Y)
which(xcol=="Y")
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm_val < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
sim_disc[which(is.na(sim_disc[,i])),i]=mtab[colnames(sim_disc)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,c(xcol,"Y")]
sim_disc=sim_disc[,c(xcol,"Y")]
# Fit RF
nt=round(dim(sim_disc)[1]/2)
m_sim=randomForest(Y~.,data=sim_disc)
# Predictions on simulated validation sample
Ypred_sim=predict(m1,sim_val,type="prob")[,1]
# Predictions on simulated validation sample
Ypred_sim=predict(m_sim,sim_val,type="prob")[,1]
# Function to run test for one potential sample size
test_n=function(nr) {
for (ss in 1:nsim) {
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm_val < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
sim_disc[which(is.na(sim_disc[,i])),i]=mtab[colnames(sim_disc)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,c(xcol,"Y")]
sim_disc=sim_disc[,c(xcol,"Y")]
# Fit RF
nt=round(dim(sim_disc)[1]/2)
m_sim=randomForest(Y~.,data=sim_disc)
# Predictions on simulated validation sample
Ypred_sim=predict(m_sim,sim_val,type="prob")[,1]
# Wilcoxon test - does the model perform better than randomly?
wt_sim=wilcox.test(Ypred_sim[which(sim_val$Y==1)],Ypred_sim[which(sim_val$Y==0)])
npoll[ss]=wt_sim$p.value
}
return(npoll)
}
nsim
nsim=10
# Run bisection method to find sample size for 90% power
power_threshold=0.9
lower=nrs[1]
upper=nrs[2]
p_lower=test_n(lower)
p_upper=test_n(upper)
# Function to run test for one potential sample size
test_n=function(nr) {
npoll=rep(NA,nsim)
for (ss in 1:nsim) {
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm_val < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
sim_disc[which(is.na(sim_disc[,i])),i]=mtab[colnames(sim_disc)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,c(xcol,"Y")]
sim_disc=sim_disc[,c(xcol,"Y")]
# Fit RF
nt=round(dim(sim_disc)[1]/2)
m_sim=randomForest(Y~.,data=sim_disc)
# Predictions on simulated validation sample
Ypred_sim=predict(m_sim,sim_val,type="prob")[,1]
# Wilcoxon test - does the model perform better than randomly?
wt_sim=wilcox.test(Ypred_sim[which(sim_val$Y==1)],Ypred_sim[which(sim_val$Y==0)])
npoll[ss]=wt_sim$p.value
}
return(npoll)
}
# Run bisection method to find sample size for 90% power
power_threshold=0.9
lower=nrs[1]
upper=nrs[2]
p_lower=test_n(lower)
p_upper=test_n(upper)
# Training data
Y=YDM
train_dat=cbind(Xall,Y=as.factor(Y))
nrs=c(100,500) # Maximum and minimum sizes to test
nsim=100 # Simulate this many times
# Store simulation data in a list
sim_data=list()
##**********************************************************************
## Sample size calculation                                          ####
##**********************************************************************
# Function to run test for one potential sample size
test_n=function(nr) {
npoll=rep(NA,nsim)
for (ss in 1:nsim) {
# Take bootstrap sample
sboot=sample(1:dim(train_dat)[1],nr,rep=TRUE);
cboot=setdiff(1:dim(train_dat)[1],sboot)
# Simulated validation data and discovery data
sim_val=train_dat[sboot,]
sim_disc=train_dat[cboot,]
# Variables with missingness in simulated validation sample <0.5
nm_val=apply(sim_val,2,function(x) length(which(is.na(x))))/dim(sim_val)[1]
subm_val=colnames(sim_val)[which(nm_val < 0.5)]
# Intersection with pre-operative predictors
xcol=intersect(intersect(subm_val,colnames(Xall)),preop_predictors)
# Mean-value impute (according to original dataset)
for (i in 1:ncol(sim_val)) {
if (colnames(sim_val)[i] %in% names(mtab)) {
sim_val[which(is.na(sim_val[,i])),i]=mtab[colnames(sim_val)[i]]
sim_disc[which(is.na(sim_disc[,i])),i]=mtab[colnames(sim_disc)[i]]
}
}
# Restrict to available predictors
sim_val=sim_val[,c(xcol,"Y")]
sim_disc=sim_disc[,c(xcol,"Y")]
# Fit RF
nt=round(dim(sim_disc)[1]/2)
m_sim=randomForest(Y~.,data=sim_disc)
# Predictions on simulated validation sample
Ypred_sim=predict(m_sim,sim_val,type="prob")[,1]
# Wilcoxon test - does the model perform better than randomly?
wt_sim=wilcox.test(Ypred_sim[which(sim_val$Y==1)],Ypred_sim[which(sim_val$Y==0)])
npoll[ss]=wt_sim$p.value
}
return(npoll)
}
# Run bisection method to find sample size for 90% power
power_threshold=0.9
lower=nrs[1]
upper=nrs[2]
p_lower=test_n(lower)
p_upper=test_n(upper)
